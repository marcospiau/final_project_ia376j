# necessary
include 'defaults.gin'

# Parameters used on T5 paper for finetuning
T5OCRBaseline.optimizer = 'adafactor'
T5OCRBaseline.learning_rate = 1e-3
T5OCRBaseline.generate_max_length = 512
# T5_FINETUNING_BATCH_SIZE = 128


# model size and task
task_train/operative_macro.x = 'extract_company'
t5_prefix = 't5-small'

# model
T5OCRBaseline.t5_model_prefix = %t5_prefix
T5OCRBaseline.t5_tokenizer_prefix = %t5_prefix
sroie_t5_baseline/get_default_preprocessing_functions.str_replace_newlines = ' '

# dataset seq lengths
get_datasets_dict_from_task_functions_map.t5_prefix = %t5_prefix
get_datasets_dict_from_task_functions_map.max_source_length = 512
get_datasets_dict_from_task_functions_map.max_target_length = 64

# batch sizes
get_dataloaders_dict_from_datasets_dict.batch_size = 32

trainer.accumulate_grad_batches = 4
trainer.max_epochs = 2
# trainer.min_epochs = 20
early_stop.patience = 50

neptune_logger.experiment_name = "newlines_as_spaces_t5_default_finetune"

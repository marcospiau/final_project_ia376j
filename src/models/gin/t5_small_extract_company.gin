# necessary
include 'defaults.gin'

# Parameters used on T5 paper for finetuning
T5OCRBaseline.optimizer = 'adafactor'
T5OCRBaseline.learning_rate = 1e-3
T5OCRBaseline.generate_max_length = 512
# T5_FINETUNING_BATCH_SIZE = 128


# model size and task
task_train/operative_macro.x = 'extract_company'
t5_prefix = 't5-small'

# model
T5OCRBaseline.t5_model_prefix = %t5_prefix
T5OCRBaseline.t5_tokenizer_prefix = %t5_prefix
sroie_t5_baseline/get_default_preprocessing_functions.str_replace_newlines = ' '

# dataset seq lengths
get_datasets_dict_from_task_functions_map.t5_prefix = %t5_prefix
get_datasets_dict_from_task_functions_map.max_source_length = 512
get_datasets_dict_from_task_functions_map.max_target_length = 64

# batch sizes
# batch_size =  %T5_DEFAULT_BATCH_SIZES[%t5_prefix]
get_dataloaders_dict_from_datasets_dict.batch_size = 32
# trainer.accumulate_grad_batches = @get_accumulate_grad_batches(%T5_FINETUNING_BATCH_SIZE, %batch_size)
trainer.accumulate_grad_batches = 4
trainer.max_epochs = 3
early_stop.patience = 50

neptune_logger.experiment_name = "tentando_rodar_com_gin"
